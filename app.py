import os
import io
import re
import time
import tempfile
from datetime import datetime, timedelta
from concurrent.futures import ThreadPoolExecutor, as_completed

import requests
import pandas as pd
import numpy as np
from openai import OpenAI
import streamlit as st

# =========================================
# Streamlit page config
# =========================================
st.set_page_config(
    page_title="BSE Company Update â€” OpenAI PDF Summarizer",
    layout="wide"
)

# =========================================
# OpenAI client init & diagnostics
# =========================================

api_key = st.secrets.get("OPENAI_API_KEY", os.getenv("OPENAI_API_KEY"))
if not api_key:
    st.error("Missing OPENAI_API_KEY (set env var or add to Streamlit secrets).")
    st.stop()

client = OpenAI(api_key=api_key)

with st.expander("ðŸ” OpenAI connection diagnostics", expanded=False):
    # 1) Is the key visible?
    key_src = "st.secrets" if "OPENAI_API_KEY" in st.secrets else "env"
    mask = lambda s: (s[:7] + "..." + s[-4:]) if s and len(s) > 12 else "unset"
    st.write("Key source:", key_src)
    st.write(
        "API key (masked):",
        mask(st.secrets.get("OPENAI_API_KEY") or os.getenv("OPENAI_API_KEY"))
    )

    # 2) Can we list models? (auth + project/perm sanity)
    try:
        _ = client.models.list()
        st.success("Models list ok â€” auth + project look good.")
    except Exception as e:
        st.error(f"Models list failed: {e}")

    # 3) Can we call a tiny Responses echo? (billing/quota often shows up here)
    try:
        r = client.responses.create(model="gpt-4.1-mini", input="ping")
        st.success("Responses call ok.")
    except Exception as e:
        st.error(f"Responses call failed: {e}")

# =========================================
# Main UI header
# =========================================
st.title("ðŸ“ˆ BSE Company Update â€” M&A / Merger / Scheme / JV (OpenAI-only)")
st.caption(
    "Fetch BSE announcements â†’ pick PDF â†’ upload to OpenAI â†’ summarize. "
    "No local NLP or OCR is used; summaries are generated by OpenAI models."
)

# =========================================
# Small utilities
# =========================================
_ILLEGAL_RX = re.compile(r'[\x00-\x08\x0b\x0c\x0e-\x1f\x7f]')

def _clean(s: str) -> str:
    return _ILLEGAL_RX.sub('', s) if isinstance(s, str) else s

def _first_col(df: pd.DataFrame, names):
    for n in names:
        if n in df.columns:
            return n
    return None

def _norm(s):
    return re.sub(r"\s+", " ", str(s or "")).strip()

def _slug(s: str, maxlen: int = 60) -> str:
    s = re.sub(r"[^A-Za-z0-9]+", "_", str(s or "")).strip("_")
    return (s[:maxlen] if len(s) > maxlen else s) or "file"

# =========================================
# Attachment URL candidates
# =========================================
def _candidate_urls(row):
    cands = []
    att = str(row.get("ATTACHMENTNAME") or "").strip()
    if att:
        cands += [
            f"https://www.bseindia.com/xml-data/corpfiling/AttachHis/{att}",
            f"https://www.bseindia.com/xml-data/corpfiling/Attach/{att}",
            f"https://www.bseindia.com/xml-data/corpfiling/AttachLive/{att}",
        ]
    ns = str(row.get("NSURL") or "").strip()
    if ".pdf" in ns.lower():
        cands.append(
            ns if ns.lower().startswith("http")
            else "https://www.bseindia.com/" + ns.lstrip("/")
        )
    seen, out = set(), []
    for u in cands:
        if u and u not in seen:
            out.append(u)
            seen.add(u)
    return out

def _primary_bse_url(row):
    """Build full BSE announcement URL from NSURL (if present)."""
    ns = str(row.get("NSURL") or "").strip()
    if not ns:
        return ""
    return ns if ns.lower().startswith("http") else "https://www.bseindia.com/" + ns.lstrip("/")

# =========================================
# BSE fetch â€” strict; returns filtered DF
# =========================================
def fetch_bse_announcements_strict(
    start_yyyymmdd: str,
    end_yyyymmdd: str,
    verbose: bool = True,
    request_timeout: int = 25
) -> pd.DataFrame:
    """Fetches raw announcements, then filters:
    Category='Company Update' AND subcategory contains any:
    Acquisition | Amalgamation / Merger | Scheme of Arrangement | Joint Venture
    """
    assert len(start_yyyymmdd) == 8 and len(end_yyyymmdd) == 8
    assert start_yyyymmdd <= end_yyyymmdd

    base_page = "https://www.bseindia.com/corporates/ann.html"
    url = "https://api.bseindia.com/BseIndiaAPI/api/AnnSubCategoryGetData/w"

    s = requests.Session()
    s.headers.update({
        "User-Agent": "Mozilla/5.0",
        "Accept": "application/json, text/plain, */*",
        "Accept-Language": "en-US,en;q=0.9",
        "Referer": base_page,
        "X-Requested-With": "XMLHttpRequest",
        "Cache-Control": "no-cache",
        "Pragma": "no-cache",
    })

    try:
        s.get(base_page, timeout=15)
    except Exception:
        pass

    variants = [
        {"subcategory": "-1", "strSearch": "P"},
        {"subcategory": "-1", "strSearch": ""},
        {"subcategory": "",   "strSearch": "P"},
        {"subcategory": "",   "strSearch": ""},
    ]

    all_rows = []
    for v in variants:
        params = {
            "pageno": 1,
            "strCat": "-1",
            "subcategory": v["subcategory"],
            "strPrevDate": start_yyyymmdd,
            "strToDate": end_yyyymmdd,
            "strSearch": v["strSearch"],
            "strscrip": "",
            "strType": "C",
        }
        rows, total, page = [], None, 1
        while True:
            r = s.get(url, params=params, timeout=request_timeout)
            ct = r.headers.get("content-type", "")
            if "application/json" not in ct:
                if verbose:
                    st.warning(
                        f"[variant {v}] non-JSON on page {page} (ct={ct})."
                    )
                break
            data = r.json()
            table = data.get("Table") or []
            rows.extend(table)
            if total is None:
                try:
                    total = int((data.get("Table1") or [{}])[0].get("ROWCNT") or 0)
                except Exception:
                    total = None
            if not table:
                break
            params["pageno"] += 1
            page += 1
            time.sleep(0.25)
            if total and len(rows) >= total:
                break
        if rows:
            all_rows = rows
            break

    if not all_rows:
        return pd.DataFrame()

    all_keys = set()
    for r in all_rows:
        all_keys.update(r.keys())
    df = pd.DataFrame(all_rows, columns=list(all_keys))

    # Filter to Company Update
    def filter_announcements(
        df_in: pd.DataFrame, category_filter="Company Update"
    ) -> pd.DataFrame:
        if df_in.empty:
            return df_in.copy()
        cat_col = _first_col(
            df_in,
            ["CATEGORYNAME", "CATEGORY", "NEWS_CAT", "NEWSCATEGORY", "NEWS_CATEGORY"],
        )
        if not cat_col:
            return df_in.copy()
        df2 = df_in.copy()
        df2["_cat_norm"] = df2[cat_col].map(lambda x: _norm(x).lower())
        return df2.loc[
            df2["_cat_norm"] == _norm(category_filter).lower()
        ].drop(columns=["_cat_norm"])

    df_filtered = filter_announcements(df, category_filter="Company Update")

    # Filter to specific subcategories
    df_filtered = df_filtered.loc[
        df_filtered.filter(
            ["NEWSSUB", "SUBCATEGORY", "SUBCATEGORYNAME", "NEWS_SUBCATEGORY", "NEWS_SUB"],
            axis=1,
        )
        .astype(str)
        .apply(
            lambda col: col.str.contains(
                r"(Acquisition|Amalgamation\s*/\s*Merger|Scheme of Arrangement|Joint Venture)",
                case=False,
                na=False,
            ),
            axis=0,
        )
        .any(axis=1)
    ]

    return df_filtered

# =========================================
# OpenAI PDF summarization helpers
# =========================================
def _download_pdf(url: str, timeout=25) -> bytes:
    s = requests.Session()
    s.headers.update({
        "User-Agent": "Mozilla/5.0",
        "Accept": "application/pdf,application/octet-stream,*/*",
        "Referer": "https://www.bseindia.com/corporates/ann.html",
        "Accept-Language": "en-US,en;q=0.9",
    })
    r = s.get(url, timeout=timeout, allow_redirects=True, stream=False)
    if r.status_code != 200:
        raise RuntimeError(f"HTTP {r.status_code}")
    data = r.content
    # Some BSE PDFs have weird headers; simple check is enough
    if not (data[:8].startswith(b"%PDF") or url.lower().endswith(".pdf")):
        pass
    return data

def _upload_to_openai(pdf_bytes: bytes, fname: str = "document.pdf"):
    # The Files API stores the PDF so a model can read it; then we attach it in a Responses call.
    with tempfile.NamedTemporaryFile(delete=False, suffix=".pdf") as tmp:
        tmp.write(pdf_bytes)
        tmp.flush()
        f = client.files.create(file=open(tmp.name, "rb"), purpose="assistants")
    return f

def _render_bordered_table_from_json(json_text: str, key: str = "table"):
    """
    Safely render a bordered table from JSON-like text.
    Auto-cleans extra backticks, markdown fences, and stray text.
    """
    import json

    cleaned = (
        json_text.strip()
        .replace("```json", "")
        .replace("```", "")
        .replace("â€œ", '"')
        .replace("â€", '"')
        .replace("â€™", "'")
    )

    # Try to find the first '{' and last '}' to isolate valid JSON
    if "{" in cleaned and "}" in cleaned:
        cleaned = cleaned[cleaned.find("{") : cleaned.rfind("}") + 1]

    try:
        data = json.loads(cleaned)
        rows = data.get(key, [])
        if not rows:
            st.warning("No rows found in parsed JSON.")
            st.code(cleaned, language="json")
            return
        df = pd.DataFrame(rows)
        styled = (
            df.style.hide(axis="index").set_table_styles(
                [
                    {
                        "selector": "table",
                        "props": "border-collapse: collapse; border: 1px solid #bbb;",
                    },
                    {
                        "selector": "th",
                        "props": (
                            "border: 1px solid #bbb; padding: 8px; "
                            "background: #f6f7fb; text-align: left;"
                        ),
                    },
                    {
                        "selector": "td",
                        "props": "border: 1px solid #bbb; padding: 8px;",
                    },
                ]
            )
        )
        st.markdown(styled.to_html(), unsafe_allow_html=True)
    except Exception as e:
        st.warning(f"Could not parse model output as JSON ({e}). Showing raw text:")
        st.code(json_text)

def summarize_pdf_with_openai(
    pdf_bytes: bytes,
    company: str,
    headline: str,
    subcat: str,
    pdf_url: str,
    news_url: str,
    market_cap: str = "NA",
    model: str = "gpt-4.1-mini",
    style: str = "bullets",
    max_output_tokens: int = 800,
    temperature: float = 0.2,
) -> str:
    """
    Uses the Responses API with a file attachment.
    The model reads the PDF and returns a structured JSON object with detailed deal analytics.
    """
    fobj = _upload_to_openai(pdf_bytes, fname=f"{_slug(company or 'doc')}.pdf")

    style_hint = (
        "Use concise bullet-like phrasing inside the JSON fields."
        if style == "bullets"
        else "Use compact sentence-style phrasing inside the JSON fields."
    )

    # JSON braces in f-string are fine since we're not nesting {} for formatting inside
    task = f"""
You are a meticulous M&A / corporate actions analyst specializing in Indian listed company filings.

Read the attached BSE/SEBI filing PDF carefully and produce a table with exactly one row and the following columns:

1) Company
2) Market Cap (â‚¹ Cr)
3) Nature of Event
4) Announcement Type From PDF
5) Key Announcement Components
6) Timeline (announcement â†’ approvals â†’ execution)
7) Stakehold
