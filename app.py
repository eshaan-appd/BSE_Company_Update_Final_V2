import os
import io
import re
import time
import tempfile
from datetime import datetime, timedelta
from concurrent.futures import ThreadPoolExecutor, as_completed

import requests
import pandas as pd
import numpy as np
from openai import OpenAI
import streamlit as st

# =========================================
# Streamlit page config
# =========================================
st.set_page_config(
    page_title="BSE Company Update ‚Äî OpenAI PDF Summarizer",
    layout="wide"
)

# =========================================
# OpenAI client init & diagnostics
# =========================================

api_key = st.secrets.get("OPENAI_API_KEY", os.getenv("OPENAI_API_KEY"))
if not api_key:
    st.error("Missing OPENAI_API_KEY (set env var or add to Streamlit secrets).")
    st.stop()

client = OpenAI(api_key=api_key)

with st.expander("üîç OpenAI connection diagnostics", expanded=False):
    # 1) Is the key visible?
    key_src = "st.secrets" if "OPENAI_API_KEY" in st.secrets else "env"
    mask = lambda s: (s[:7] + "..." + s[-4:]) if s and len(s) > 12 else "unset"
    st.write("Key source:", key_src)
    st.write(
        "API key (masked):",
        mask(st.secrets.get("OPENAI_API_KEY") or os.getenv("OPENAI_API_KEY"))
    )

    # 2) Can we list models? (auth + project/perm sanity)
    try:
        _ = client.models.list()
        st.success("Models list ok ‚Äî auth + project look good.")
    except Exception as e:
        st.error(f"Models list failed: {e}")

    # 3) Can we call a tiny Responses echo? (billing/quota often shows up here)
    try:
        r = client.responses.create(model="gpt-4.1-mini", input="ping")
        st.success("Responses call ok.")
    except Exception as e:
        st.error(f"Responses call failed: {e}")

# =========================================
# Main UI header
# =========================================
st.title("üìà BSE Company Update ‚Äî M&A / Merger / Scheme / JV (OpenAI-only)")
st.caption(
    "Fetch BSE announcements ‚Üí pick PDF ‚Üí upload to OpenAI ‚Üí summarize. "
    "No local NLP or OCR is used; summaries are generated by OpenAI models."
)

# =========================================
# Small utilities
# =========================================
_ILLEGAL_RX = re.compile(r'[\x00-\x08\x0b\x0c\x0e-\x1f\x7f]')

def _clean(s: str) -> str:
    return _ILLEGAL_RX.sub('', s) if isinstance(s, str) else s

def _first_col(df: pd.DataFrame, names):
    for n in names:
        if n in df.columns:
            return n
    return None

def _norm(s):
    return re.sub(r"\s+", " ", str(s or "")).strip()

def _slug(s: str, maxlen: int = 60) -> str:
    s = re.sub(r"[^A-Za-z0-9]+", "_", str(s or "")).strip("_")
    return (s[:maxlen] if len(s) > maxlen else s) or "file"

# =========================================
# Attachment URL candidates
# =========================================
def _candidate_urls(row):
    cands = []
    att = str(row.get("ATTACHMENTNAME") or "").strip()
    if att:
        cands += [
            f"https://www.bseindia.com/xml-data/corpfiling/AttachHis/{att}",
            f"https://www.bseindia.com/xml-data/corpfiling/Attach/{att}",
            f"https://www.bseindia.com/xml-data/corpfiling/AttachLive/{att}",
        ]
    ns = str(row.get("NSURL") or "").strip()
    if ".pdf" in ns.lower():
        cands.append(
            ns if ns.lower().startswith("http")
            else "https://www.bseindia.com/" + ns.lstrip("/")
        )
    seen, out = set(), []
    for u in cands:
        if u and u not in seen:
            out.append(u)
            seen.add(u)
    return out

def _primary_bse_url(row):
    """Build full BSE announcement URL from NSURL (if present)."""
    ns = str(row.get("NSURL") or "").strip()
    if not ns:
        return ""
    return ns if ns.lower().startswith("http") else "https://www.bseindia.com/" + ns.lstrip("/")

# =========================================
# BSE fetch ‚Äî strict; returns filtered DF
# =========================================
def fetch_bse_announcements_strict(
    start_yyyymmdd: str,
    end_yyyymmdd: str,
    verbose: bool = True,
    request_timeout: int = 25
) -> pd.DataFrame:
    """Fetches raw announcements, then filters:
    Category='Company Update' AND subcategory contains any:
    Acquisition | Amalgamation / Merger | Scheme of Arrangement | Joint Venture
    """
    assert len(start_yyyymmdd) == 8 and len(end_yyyymmdd) == 8
    assert start_yyyymmdd <= end_yyyymmdd

    base_page = "https://www.bseindia.com/corporates/ann.html"
    url = "https://api.bseindia.com/BseIndiaAPI/api/AnnSubCategoryGetData/w"

    s = requests.Session()
    s.headers.update({
        "User-Agent": "Mozilla/5.0",
        "Accept": "application/json, text/plain, */*",
        "Accept-Language": "en-US,en;q=0.9",
        "Referer": base_page,
        "X-Requested-With": "XMLHttpRequest",
        "Cache-Control": "no-cache",
        "Pragma": "no-cache",
    })

    try:
        s.get(base_page, timeout=15)
    except Exception:
        pass

    variants = [
        {"subcategory": "-1", "strSearch": "P"},
        {"subcategory": "-1", "strSearch": ""},
        {"subcategory": "",   "strSearch": "P"},
        {"subcategory": "",   "strSearch": ""},
    ]

    all_rows = []
    for v in variants:
        params = {
            "pageno": 1,
            "strCat": "-1",
            "subcategory": v["subcategory"],
            "strPrevDate": start_yyyymmdd,
            "strToDate": end_yyyymmdd,
            "strSearch": v["strSearch"],
            "strscrip": "",
            "strType": "C",
        }
        rows, total, page = [], None, 1
        while True:
            r = s.get(url, params=params, timeout=request_timeout)
            ct = r.headers.get("content-type", "")
            if "application/json" not in ct:
                if verbose:
                    st.warning(
                        f"[variant {v}] non-JSON on page {page} (ct={ct})."
                    )
                break
            data = r.json()
            table = data.get("Table") or []
            rows.extend(table)
            if total is None:
                try:
                    total = int((data.get("Table1") or [{}])[0].get("ROWCNT") or 0)
                except Exception:
                    total = None
            if not table:
                break
            params["pageno"] += 1
            page += 1
            time.sleep(0.25)
            if total and len(rows) >= total:
                break
        if rows:
            all_rows = rows
            break

    if not all_rows:
        return pd.DataFrame()

    all_keys = set()
    for r in all_rows:
        all_keys.update(r.keys())
    df = pd.DataFrame(all_rows, columns=list(all_keys))

    # Filter to Company Update
    def filter_announcements(
        df_in: pd.DataFrame, category_filter="Company Update"
    ) -> pd.DataFrame:
        if df_in.empty:
            return df_in.copy()
        cat_col = _first_col(
            df_in,
            ["CATEGORYNAME", "CATEGORY", "NEWS_CAT", "NEWSCATEGORY", "NEWS_CATEGORY"],
        )
        if not cat_col:
            return df_in.copy()
        df2 = df_in.copy()
        df2["_cat_norm"] = df2[cat_col].map(lambda x: _norm(x).lower())
        return df2.loc[
            df2["_cat_norm"] == _norm(category_filter).lower()
        ].drop(columns=["_cat_norm"])

    df_filtered = filter_announcements(df, category_filter="Company Update")

    # Filter to specific subcategories
    df_filtered = df_filtered.loc[
        df_filtered.filter(
            ["NEWSSUB", "SUBCATEGORY", "SUBCATEGORYNAME", "NEWS_SUBCATEGORY", "NEWS_SUB"],
            axis=1,
        )
        .astype(str)
        .apply(
            lambda col: col.str.contains(
                r"(Acquisition|Amalgamation\s*/\s*Merger|Scheme of Arrangement|Joint Venture)",
                case=False,
                na=False,
            ),
            axis=0,
        )
        .any(axis=1)
    ]

    return df_filtered

# =========================================
# OpenAI PDF summarization helpers
# =========================================
def _download_pdf(url: str, timeout=25) -> bytes:
    s = requests.Session()
    s.headers.update({
        "User-Agent": "Mozilla/5.0",
        "Accept": "application/pdf,application/octet-stream,*/*",
        "Referer": "https://www.bseindia.com/corporates/ann.html",
        "Accept-Language": "en-US,en;q=0.9",
    })
    r = s.get(url, timeout=timeout, allow_redirects=True, stream=False)
    if r.status_code != 200:
        raise RuntimeError(f"HTTP {r.status_code}")
    data = r.content
    # Some BSE PDFs have weird headers; simple check is enough
    if not (data[:8].startswith(b"%PDF") or url.lower().endswith(".pdf")):
        pass
    return data

def _upload_to_openai(pdf_bytes: bytes, fname: str = "document.pdf"):
    # The Files API stores the PDF so a model can read it; then we attach it in a Responses call.
    with tempfile.NamedTemporaryFile(delete=False, suffix=".pdf") as tmp:
        tmp.write(pdf_bytes)
        tmp.flush()
        f = client.files.create(file=open(tmp.name, "rb"), purpose="assistants")
    return f

def _render_bordered_table_from_json(json_text: str, key: str = "table"):
    """
    Safely render a bordered table from JSON-like text.
    Auto-cleans extra backticks, markdown fences, and stray text.
    """
    import json

    cleaned = (
        json_text.strip()
        .replace("```json", "")
        .replace("```", "")
        .replace("‚Äú", '"')
        .replace("‚Äù", '"')
        .replace("‚Äô", "'")
    )

    # Try to find the first '{' and last '}' to isolate valid JSON
    if "{" in cleaned and "}" in cleaned:
        cleaned = cleaned[cleaned.find("{") : cleaned.rfind("}") + 1]

    try:
        data = json.loads(cleaned)
        rows = data.get(key, [])
        if not rows:
            st.warning("No rows found in parsed JSON.")
            st.code(cleaned, language="json")
            return
        df = pd.DataFrame(rows)
        styled = (
            df.style.hide(axis="index").set_table_styles(
                [
                    {
                        "selector": "table",
                        "props": "border-collapse: collapse; border: 1px solid #bbb;",
                    },
                    {
                        "selector": "th",
                        "props": (
                            "border: 1px solid #bbb; padding: 8px; "
                            "background: #f6f7fb; text-align: left;"
                        ),
                    },
                    {
                        "selector": "td",
                        "props": "border: 1px solid #bbb; padding: 8px;",
                    },
                ]
            )
        )
        st.markdown(styled.to_html(), unsafe_allow_html=True)
    except Exception as e:
        st.warning(f"Could not parse model output as JSON ({e}). Showing raw text:")
        st.code(json_text)

def summarize_pdf_with_openai(
    pdf_bytes: bytes,
    company: str,
    headline: str,
    subcat: str,
    pdf_url: str,
    news_url: str,
    market_cap: str = "NA",
    model: str = "gpt-4.1-mini",
    style: str = "bullets",
    max_output_tokens: int = 800,
    temperature: float = 0.2,
) -> str:
    """
    Uses the Responses API with a file attachment.
    The model reads the PDF and returns a structured JSON object with detailed deal analytics.
    """
    fobj = _upload_to_openai(pdf_bytes, fname=f"{_slug(company or 'doc')}.pdf")

    style_hint = (
        "Use concise bullet-like phrasing inside the JSON fields."
        if style == "bullets"
        else "Use compact sentence-style phrasing inside the JSON fields."
    )

    # JSON braces in f-string are fine since we're not nesting {} for formatting inside
    task = f"""
You are a meticulous M&A / corporate actions analyst specializing in Indian listed company filings.

Read the attached BSE/SEBI filing PDF carefully and produce a table with exactly one row and the following columns:

1) Company
2) Market Cap (‚Çπ Cr)
3) Nature of Event
4) Announcement Type From PDF
5) Key Announcement Components
6) Timeline (announcement ‚Üí approvals ‚Üí execution)
7) Stakeholder Impact
8) Valuation Impact
9) Expected Triggers Ahead
10) Risks
11) Probability Assessment
12) Regulations
13) Source Documents

Column definitions and guidance:

- "Company":
  ‚Ä¢ Copy the company name from the provided context below (do not invent a new name).

- "Market Cap (‚Çπ Cr)":
  ‚Ä¢ If the market cap is explicitly mentioned in the filing or you can clearly infer it, put that value (numeric or short string).
  ‚Ä¢ Otherwise, return exactly "{market_cap}" (the default context value).

- "Nature of Event":
  ‚Ä¢ High-level classification such as:
    "Acquisition", "Business Transfer", "Amalgamation / Merger", "Demerger",
    "Joint Venture", "Slump Sale", "Scheme of Arrangement", "Preferential Issue",
    "QIP", "Buyback", "ESOP / ESOS", etc.

- "Announcement Type From PDF":
  ‚Ä¢ Concise, specific title of the announcement, e.g.:
    "Outcome of Board Meeting", "Intimation of Board Meeting",
    "Scheme of Amalgamation", "Approval of Scheme by NCLT",
    "Execution of Share Purchase Agreement", etc.

- "Key Announcement Components":
  ‚Ä¢ 2‚Äì5 short bullet-style points capturing the core economic/strategic elements:
    counterparty, size, structure, consideration, key conditions, regulatory aspects.

- "Timeline (announcement ‚Üí approvals ‚Üí execution)":
  ‚Ä¢ Narrative or bullet-style sequence from:
    initial announcement ‚Üí board/shareholder/regulatory approvals ‚Üí expected completion.
  ‚Ä¢ If future dates are mentioned, include them; otherwise, give an indicative/relative description.

- "Stakeholder Impact":
  ‚Ä¢ Briefly assess impact on key stakeholders:
    shareholders, lenders, employees, customers, regulators, promoters.

- "Valuation Impact":
  ‚Ä¢ Comment on potential valuation implication:
    EPS impact, leverage, ROE, growth visibility, multiple re-rating, overhangs.
  ‚Ä¢ If quantitative details are available (deal value, EV, P/E, etc.), briefly refer to them.
  ‚Ä¢ If no meaningful comment is possible, say "Not enough information".

- "Expected Triggers Ahead":
  ‚Ä¢ List 2‚Äì5 key upcoming milestones or triggers investors should track
    (regulatory approvals, completion timelines, integration progress, further disclosures, etc.).

- "Risks":
  ‚Ä¢ 2‚Äì5 concise risk points (regulatory, execution, financing, integration, dilution, etc.).

- "Probability Assessment":
  ‚Ä¢ Classify overall likelihood of successful completion of the core event as one of:
    "Low", "Medium", or "High".

- "Regulations":
  ‚Ä¢ If the PDF explicitly cites regulations, reproduce them exactly
    (e.g. "Regulation 30 of SEBI (LODR) Regulations, 2015").
  ‚Ä¢ Otherwise, infer likely applicable regulations (e.g. Reg. 30, Reg. 33, SEBI (PIT), SEBI (ICDR)) and list them.

- "Source Documents":
  ‚Ä¢ Use exactly the following URLs from context:
    PDF: {pdf_url or 'NA'}
    BSE Announcement: {news_url or 'NA'}
  ‚Ä¢ Return them in a single string, e.g.:
    "PDF: {pdf_url or 'NA'}; BSE: {news_url or 'NA'}".

{style_hint}

Output format ‚Äî return ONLY valid JSON with this exact structure (no prose, no markdown):

{{
  "table": [
    {{
      "Company": "{company or 'NA'}",
      "Market Cap (‚Çπ Cr)": "{market_cap}",
      "Nature of Event": "<nature>",
      "Announcement Type From PDF": "<announcement type>",
      "Key Announcement Components": "<key components>",
      "Timeline (announcement ‚Üí approvals ‚Üí execution)": "<timeline>",
      "Stakeholder Impact": "<stakeholder impact>",
      "Valuation Impact": "<valuation impact>",
      "Expected Triggers Ahead": "<expected triggers>",
      "Risks": "<risks>",
      "Probability Assessment": "<Low|Medium|High>",
      "Regulations": "<regulation text>",
      "Source Documents": "PDF: {pdf_url or 'NA'}; BSE: {news_url or 'NA'}"
    }}
  ]
}}

Context:
Company: {company or 'NA'}
Headline: {headline or 'NA'}
Subcategory: {subcat or 'NA'}
PDF URL: {pdf_url or 'NA'}
BSE Announcement URL: {news_url or 'NA'}
"""

    resp = client.responses.create(
        model=model,
        temperature=temperature,
        max_output_tokens=max_output_tokens,
        input=[{
            "role": "user",
            "content": [
                {"type": "input_text", "text": task},
                {"type": "input_file", "file_id": fobj.id},
            ],
        }],
    )
    return (resp.output_text or "").strip()

def safe_summarize(*args, **kwargs) -> str:
    """Simple rate-limit-friendly wrapper around summarize_pdf_with_openai."""
    for i in range(4):
        try:
            return summarize_pdf_with_openai(*args, **kwargs)
        except Exception as e:
            msg = str(e)
            if "429" in msg or "rate" in msg.lower():
                time.sleep(2.0 * (i + 1))
                continue
            raise
    return "‚ö†Ô∏è Unable to summarize due to repeated rate limits."

# =========================================
# Sidebar controls
# =========================================
with st.sidebar:
    st.header("‚öôÔ∏è Controls")
    today = datetime.now().date()
    start_date = st.date_input(
        "Start date",
        value=today - timedelta(days=1),
        max_value=today
    )
    end_date = st.date_input(
        "End date",
        value=today,
        max_value=today,
        min_value=start_date
    )

    model = st.selectbox(
        "OpenAI model",
        ["gpt-4.1-mini", "gpt-4o-mini", "gpt-4.1"],
        index=0,
        help="Models with vision/file-reading capability. 4.1-mini/4o-mini are cost-efficient."
    )
    style = st.radio("Summary style", ["bullets", "narrative"], horizontal=True)
    max_tokens = st.slider("Max output tokens", 200, 2000, 800, step=50)
    temperature = st.slider("Creativity (temperature)", 0.0, 1.0, 0.2, step=0.1)

    max_workers = st.slider(
        "Parallel summaries", 1, 8, 3,
        help="Lower if you hit 429s."
    )
    max_items = st.slider(
        "Max announcements to summarize", 5, 200, 60, step=5
    )

    run = st.button("üöÄ Fetch & Summarize with OpenAI", type="primary")

# =========================================
# Run pipeline (fetch ‚Üí PDFs ‚Üí OpenAI summaries)
# =========================================
def _fmt(d: datetime.date) -> str:
    return d.strftime("%Y%m%d")

def _pick_company_cols(df: pd.DataFrame) -> tuple[str, str]:
    nm = _first_col(df, ["SLONGNAME", "SNAME", "SC_NAME", "COMPANYNAME"]) or "SLONGNAME"
    subcol = _first_col(
        df,
        ["SUBCATEGORYNAME", "SUBCATEGORY", "SUB_CATEGORY", "NEWS_SUBCATEGORY"]
    ) or "SUBCATEGORYNAME"
    return nm, subcol

if run:
    if not os.getenv("OPENAI_API_KEY") and "OPENAI_API_KEY" not in st.secrets:
        st.error(
            "Missing OPENAI_API_KEY (set env var, add to Streamlit Secrets, or export in your shell)."
        )
        st.stop()

    start_str, end_str = _fmt(start_date), _fmt(end_date)

    with st.status("Fetching announcements‚Ä¶", expanded=True):
        df_hits = fetch_bse_announcements_strict(
            start_str, end_str, verbose=False
        )
        st.write(f"Matched rows after filters: **{len(df_hits)}**")

    if df_hits.empty:
        st.warning("No matching announcements in this window.")
        st.stop()

    if len(df_hits) > max_items:
        df_hits = df_hits.head(max_items)

    # Build list of PDF targets
    rows = []
    for _, r in df_hits.iterrows():
        urls = _candidate_urls(r)
        rows.append((r, urls))

    st.subheader("üìë Summaries (OpenAI)")
    nm, subcol = _pick_company_cols(df_hits)

    # Worker to download and summarize a single row
    def worker(idx, row, urls):
        # try urls in order until one downloads
        pdf_bytes, used_url = None, ""
        for u in urls:
            try:
                data = _download_pdf(u, timeout=25)
                if data and len(data) > 500:
                    pdf_bytes, used_url = data, u
                    break
            except Exception:
                continue

        if not pdf_bytes:
            return idx, used_url, "‚ö†Ô∏è Could not fetch a valid PDF.", None

        company = str(row.get(nm) or "").strip()
        headline = str(row.get("HEADLINE") or "").strip()
        subcat = str(row.get(subcol) or "").strip()
        news_url = _primary_bse_url(row)

        # Placeholder for now; plug in real market-cap fetch if desired
        market_cap = "NA"

        summary = safe_summarize(
            pdf_bytes,
            company,
            headline,
            subcat,
            used_url,          # pdf_url
            news_url,          # exchange URL
            market_cap=market_cap,
            model=model,
            style=("bullets" if style == "bullets" else "narrative"),
            max_output_tokens=int(max_tokens),
            temperature=float(temperature),
        )
        return idx, used_url, summary, None

    # Run with limited parallelism
    with ThreadPoolExecutor(max_workers=max_workers) as ex:
        futs = [
            ex.submit(worker, i, r, urls)
            for i, (r, urls) in enumerate(rows)
        ]
        for fut in as_completed(futs):
            i, pdf_url, summary, _ = fut.result()
            r = rows[i][0]
            company = str(r.get(nm) or "").strip()
            dt = str(r.get("NEWS_DT") or "").strip()
            subcat = str(r.get(subcol) or "").strip()
            headline = str(r.get("HEADLINE") or "").strip()

            # Optional context header per summary (uncomment if you like):
            # st.markdown(f"### {company}")
            # st.caption(f"{dt} ¬∑ {headline} ¬∑ {subcat}")
            _render_bordered_table_from_json(summary, key="table")

else:
    st.info(
        "Pick your date range and click **Fetch & Summarize with OpenAI**. "
        "This version uploads each PDF to OpenAI and renders the model‚Äôs "
        "structured summary (with event nature, timeline, valuation impact, "
        "triggers, risks, probability, and source docs) right here."
    )
