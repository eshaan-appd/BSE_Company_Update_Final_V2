import os
import io
import re
import time
import tempfile
from datetime import datetime, timedelta
from concurrent.futures import ThreadPoolExecutor, as_completed

import requests
import pandas as pd
import numpy as np
from openai import OpenAI
import streamlit as st

# =========================================
# Streamlit page config
# =========================================
st.set_page_config(
    page_title="BSE Company Update ‚Äî OpenAI PDF Summarizer",
    layout="wide"
)

# =========================================
# OpenAI client init & diagnostics
# =========================================

# üîß use the normal secret name; change if your secrets key is different
api_key = st.secrets.get("OPENAI_API_KEY", os.getenv("OPENAI_API_KEY"))
if not api_key:
    st.error("Missing OPENAI_API_KEY (set env var or add to Streamlit secrets).")
    st.stop()

client = OpenAI(api_key=api_key)

with st.expander("üîç OpenAI connection diagnostics", expanded=False):
    # 1) Is the key visible?
    key_src = "st.secrets" if "OPENAI_API_KEY" in st.secrets else "env"
    mask = lambda s: (s[:7] + "..." + s[-4:]) if s and len(s) > 12 else "unset"
    st.write("Key source:", key_src)
    st.write(
        "API key (masked):",
        mask(st.secrets.get("OPENAI_API_KEY") or os.getenv("OPENAI_API_KEY"))
    )

    # 2) Can we list models? (auth + project/perm sanity)
    try:
        _ = client.models.list()
        st.success("Models list ok ‚Äî auth + project look good.")
    except Exception as e:
        st.error(f"Models list failed: {e}")

    # 3) Can we call a tiny Responses echo? (billing/quota often shows up here)
    try:
        r = client.responses.create(model="gpt-4.1-mini", input="ping")
        st.success("Responses call ok.")
    except Exception as e:
        st.error(f"Responses call failed: {e}")

# =========================================
# Main UI header
# =========================================
st.title("üìà BSE Company Update ‚Äî M&A / Merger / Scheme / JV (OpenAI-only)")
st.caption(
    "Fetch BSE announcements ‚Üí pick PDF ‚Üí upload to OpenAI ‚Üí summarize. "
    "No local NLP or OCR is used; summaries are generated by OpenAI models."
)

# =========================================
# Small utilities
# =========================================
_ILLEGAL_RX = re.compile(r'[\x00-\x08\x0b\x0c\x0e-\x1f\x7f]')

def _clean(s: str) -> str:
    return _ILLEGAL_RX.sub('', s) if isinstance(s, str) else s

def _first_col(df: pd.DataFrame, names):
    for n in names:
        if n in df.columns:
            return n
    return None

def _norm(s):
    return re.sub(r"\s+", " ", str(s or "")).strip()

def _slug(s: str, maxlen: int = 60) -> str:
    s = re.sub(r"[^A-Za-z0-9]+", "_", str(s or "")).strip("_")
    return (s[:maxlen] if len(s) > maxlen else s) or "file"

# =========================================
# Attachment URL candidates
# =========================================
def _candidate_urls(row):
    cands = []
    att = str(row.get("ATTACHMENTNAME") or "").strip()
    if att:
        cands += [
            f"https://www.bseindia.com/xml-data/corpfiling/AttachHis/{att}",
            f"https://www.bseindia.com/xml-data/corpfiling/Attach/{att}",
            f"https://www.bseindia.com/xml-data/corpfiling/AttachLive/{att}",
        ]
    ns = str(row.get("NSURL") or "").strip()
    if ".pdf" in ns.lower():
        cands.append(
            ns if ns.lower().startswith("http")
            else "https://www.bseindia.com/" + ns.lstrip("/")
        )
    seen, out = set(), []
    for u in cands:
        if u and u not in seen:
            out.append(u)
            seen.add(u)
    return out

def _primary_bse_url(row):
    """Build full BSE announcement URL from NSURL (if present)."""
    ns = str(row.get("NSURL") or "").strip()
    if not ns:
        return ""
    return ns if ns.lower().startswith("http") else "https://www.bseindia.com/" + ns.lstrip("/")

# =========================================
# BSE fetch ‚Äî strict; returns filtered DF
# =========================================
def fetch_bse_announcements_strict(
    start_yyyymmdd: str,
    end_yyyymmdd: str,
    verbose: bool = True,
    request_timeout: int = 25
) -> pd.DataFrame:
    """Fetches raw announcements, then filters:
    Category='Company Update' AND subcategory contains any:
    Acquisition | Amalgamation / Merger | Scheme of Arrangement | Joint Venture
    """
    assert len(start_yyyymmdd) == 8 and len(end_yyyymmdd) == 8
    assert start_yyyymmdd <= end_yyyymmdd

    base_page = "https://www.bseindia.com/corporates/ann.html"
    url = "https://api.bseindia.com/BseIndiaAPI/api/AnnSubCategoryGetData/w"

    s = requests.Session()
    s.headers.update({
        "User-Agent": "Mozilla/5.0",
        "Accept": "application/json, text/plain, */*",
        "Accept-Language": "en-US,en;q=0.9",
        "Referer": base_page,
        "X-Requested-With": "XMLHttpRequest",
        "Cache-Control": "no-cache",
        "Pragma": "no-cache",
    })

    try:
        s.get(base_page, timeout=15)
    except Exception:
        pass

    variants = [
        {"subcategory": "-1", "strSearch": "P"},
        {"subcategory": "-1", "strSearch": ""},
        {"subcategory": "",   "strSearch": "P"},
        {"subcategory": "",   "strSearch": ""},
    ]

    all_rows = []
    for v in variants:
        params = {
            "pageno": 1,
            "strCat": "-1",
            "subcategory": v["subcategory"],
            "strPrevDate": start_yyyymmdd,
            "strToDate": end_yyyymmdd,
            "strSearch": v["strSearch"],
            "strscrip": "",
            "strType": "C",
        }
        rows, total, page = [], None, 1
        while True:
            r = s.get(url, params=params, timeout=request_timeout)
            ct = r.headers.get("content-type", "")
            if "application/json" not in ct:
                if verbose:
                    st.warning(
                        f"[variant {v}] non-JSON on page {page} (ct={ct})."
                    )
                break
            data = r.json()
            table = data.get("Table") or []
            rows.extend(table)
            if total is None:
                try:
                    total = int((data.get("Table1") or [{}])[0].get("ROWCNT") or 0)
                except Exception:
                    total = None
            if not table:
                break
            params["pageno"] += 1
            page += 1
            time.sleep(0.25)
            if total and len(rows) >= total:
                break
        if rows:
            all_rows = rows
            break

    if not all_rows:
        return pd.DataFrame()

    all_keys = set()
    for r in all_rows:
        all_keys.update(r.keys())
    df = pd.DataFrame(all_rows, columns=list(all_keys))

    # Filter to Company Update
    def filter_announcements(
        df_in: pd.DataFrame, category_filter="Company Update"
    ) -> pd.DataFrame:
        if df_in.empty:
            return df_in.copy()
        cat_col = _first_col(
            df_in,
            ["CATEGORYNAME", "CATEGORY", "NEWS_CAT", "NEWSCATEGORY", "NEWS_CATEGORY"],
        )
        if not cat_col:
            return df_in.copy()
        df2 = df_in.copy()
        df2["_cat_norm"] = df2[cat_col].map(lambda x: _norm(x).lower())
        return df2.loc[
            df2["_cat_norm"] == _norm(category_filter).lower()
        ].drop(columns=["_cat_norm"])

    df_filtered = filter_announcements(df, category_filter="Company Update")

    # Filter to specific subcategories
    df_filtered = df_filtered.loc[
        df_filtered.filter(
            ["NEWSSUB", "SUBCATEGORY", "SUBCATEGORYNAME", "NEWS_SUBCATEGORY", "NEWS_SUB"],
            axis=1,
        )
        .astype(str)
        .apply(
            lambda col: col.str.contains(
                r"(Acquisition|Amalgamation\s*/\s*Merger|Scheme of Arrangement|Joint Venture)",
                case=False,
                na=False,
            ),
            axis=0,
        )
        .any(axis=1)
    ]

    return df_filtered

# =========================================
# OpenAI PDF summarization helpers
# =========================================
def _download_pdf(url: str, timeout=25) -> bytes:
    s = requests.Session()
    s.headers.update({
        "User-Agent": "Mozilla/5.0",
        "Accept": "application/pdf,application/octet-stream,*/*",
        "Referer": "https://www.bseindia.com/corporates/ann.html",
        "Accept-Language": "en-US,en;q=0.9",
    })
    r = s.get(url, timeout=timeout, allow_redirects=True, stream=False)
    if r.status_code != 200:
        raise RuntimeError(f"HTTP {r.status_code}")
    data = r.content
    # Some BSE PDFs have weird headers; simple check is enough
    if not (data[:8].startswith(b"%PDF") or url.lower().endswith(".pdf")):
        pass
    return data

def _upload_to_openai(pdf_bytes: bytes, fname: str = "document.pdf"):
    # The Files API stores the PDF so a model can read it; then we attach it in a Responses call.
    with tempfile.NamedTemporaryFile(delete=False, suffix=".pdf") as tmp:
        tmp.write(pdf_bytes)
        tmp.flush()
        f = client.files.create(file=open(tmp.name, "rb"), purpose="assistants")
    return f

def _parse_table_from_json(json_text: str, key: str = "table") -> pd.DataFrame | None:
    """Parse the JSON text into a DataFrame without rendering."""
    import json

    cleaned = (
        json_text.strip()
        .replace("```json", "")
        .replace("```", "")
        .replace("‚Äú", '"')
        .replace("‚Äù", '"')
        .replace("‚Äô", "'")
    )

    # Try to find the first '{' and last '}' to isolate valid JSON
    if "{" in cleaned and "}" in cleaned:
        cleaned = cleaned[cleaned.find("{") : cleaned.rfind("}") + 1]

    try:
        data = json.loads(cleaned)
    except Exception:
        return None

    rows = data.get(key, [])
    if not rows:
        return None
    return pd.DataFrame(rows)

def _render_bordered_table_from_json(json_text: str, key: str = "table"):
    """
    Safely render a bordered table from JSON-like text.
    Auto-cleans extra backticks, markdown fences, and stray text.
    """
    df = _parse_table_from_json(json_text, key=key)
    if df is None or df.empty:
        st.warning("No rows found in parsed JSON.")
        st.code(json_text)
        return

    styled = (
        df.style.hide(axis="index").set_table_styles(
            [
                {
                    "selector": "table",
                    "props": "border-collapse: collapse; border: 1px solid #bbb;",
                },
                {
                    "selector": "th",
                    "props": (
                        "border: 1px solid #bbb; padding: 8px; "
                        "background: #f6f7fb; text-align: left;"
                    ),
                },
                {
                    "selector": "td",
                    "props": "border: 1px solid #bbb; padding: 8px;",
                },
            ]
        )
    )
    st.markdown(styled.to_html(), unsafe_allow_html=True)

def summarize_pdf_with_openai(
    pdf_bytes: bytes,
    company: str,
    headline: str,
    subcat: str,
    pdf_url: str,
    news_url: str,
    market_cap: str = "NA",
    model: str = "gpt-4.1-mini",
    style: str = "bullets",
    max_output_tokens: int = 800,
    temperature: float = 0.2,
) -> str:
    """
    Uses the Responses API with a file attachment.
    The model reads the PDF and returns a structured JSON object with detailed deal analytics.
    Now also uses web_search to fetch Market Cap and fill missing key data where needed.
    """
    fobj = _upload_to_openai(pdf_bytes, fname=f"{_slug(company or 'doc')}.pdf")

    style_hint = (
        "Use concise bullet-like phrasing inside the JSON fields."
        if style == "bullets"
        else "Use compact sentence-style phrasing inside the JSON fields."
    )

    task = f"""
You are a meticulous M&A / corporate actions analyst specializing in Indian listed company filings.

You have access to a web_search tool. Use it when:
- You need the latest market capitalisation of the company and it is not clearly present in the PDF.
- You need to supplement missing basic deal details (e.g., deal size, counterparty, listing ticker) that are not clearly available in the PDF.
Always treat the PDF as the primary / authoritative source. Use web_search only to:
- Fill gaps when the PDF is ambiguous or silent; and
- Cross-check obvious facts, without contradicting explicit statements in the PDF.

Web search guidance:
- For market cap, search something like:
  "Market cap {company} screener.in" OR "Market capitalization {company} BSE" OR "Market capitalization {company} NSE"
- Prefer reliable sites (stock exchanges, Screener, Moneycontrol, company filings, etc.).
- Use the latest available market cap for the main Indian listed entity.
- Express the final Market Cap in INR crore (‚Çπ Cr). If the source is in INR but not in crore, convert appropriately (1 Cr = 10,000,000).
- If web_search fails repeatedly or you cannot get a reliable market cap, return exactly "NA" for Market Cap.

Read the attached BSE/SEBI filing PDF carefully and produce a table with exactly one row and the following columns:

1) Company
2) Market Cap (‚Çπ Cr)
3) Nature of Event
4) Announcement Type From PDF
5) Key Announcement Components
6) Timeline (announcement ‚Üí approvals ‚Üí execution)
7) Stakeholder Impact
8) Valuation Impact
9) Expected Triggers Ahead
10) Risks
11) Probability Assessment
12) Regulations
13) Source Documents

Column definitions and guidance:

- "Company":
  ‚Ä¢ Copy the company name from the provided context below (do not invent a new name).
  ‚Ä¢ If you believe the legal name in the PDF slightly differs from the context (e.g., abbreviations), align with the PDF version.

- "Market Cap (‚Çπ Cr)":
  ‚Ä¢ First, check if the market cap is explicitly mentioned in the PDF.
  ‚Ä¢ If not clearly available, you MUST call web_search to fetch the latest reliable market cap for the listed company.
  ‚Ä¢ Convert the figure to INR Crore (‚Çπ Cr) if needed.
  ‚Ä¢ Return a concise numeric value or short string (e.g., "4,500").
  ‚Ä¢ If, after attempting web_search, you still cannot confidently determine the market cap, return exactly "NA".

- "Nature of Event":
  ‚Ä¢ High-level classification such as:
    "Acquisition", "Business Transfer", "Amalgamation / Merger", "Demerger",
    "Joint Venture", "Slump Sale", "Scheme of Arrangement", "Preferential Issue",
    "QIP", "Buyback", "ESOP / ESOS", etc.

- "Announcement Type From PDF":
  ‚Ä¢ Concise, specific title of the announcement, e.g.:
    "Outcome of Board Meeting", "Intimation of Board Meeting",
    "Scheme of Amalgamation", "Approval of Scheme by NCLT",
    "Execution of Share Purchase Agreement", etc.

- "Key Announcement Components":
  ‚Ä¢ 2‚Äì5 short bullet-style points capturing the core economic/strategic elements:
    counterparty, size, structure, consideration, key conditions, regulatory aspects.
  ‚Ä¢ If the PDF is vague on deal size or structure and you cannot determine it even via web_search, explicitly say "Deal size not clearly disclosed".

- "Timeline (announcement ‚Üí approvals ‚Üí execution)":
  ‚Ä¢ Narrative or bullet-style sequence from:
    initial announcement ‚Üí board/shareholder/regulatory approvals ‚Üí expected completion.
  ‚Ä¢ If future dates are mentioned, include them; otherwise, give an indicative/relative description.

- "Stakeholder Impact":
  ‚Ä¢ Briefly assess impact on key stakeholders:
    shareholders, lenders, employees, customers, regulators, promoters.

- "Valuation Impact":
  ‚Ä¢ Comment on potential valuation implication:
    EPS impact, leverage, ROE, growth visibility, multiple re-rating, overhangs.
  ‚Ä¢ If quantitative details are available (deal value, EV, P/E, etc.), briefly refer to them.
  ‚Ä¢ Even if explicit numbers are not provided, use your own expert judgement based on:
    ‚Äì the nature and size of the event (acquisition, merger, slump sale, QIP, etc.),
    ‚Äì its impact on leverage, equity dilution, goodwill/intangibles and balance sheet strength,
    ‚Äì expected changes to revenue growth, margins, business mix and risk profile,
    and provide a directional, qualitative assessment (e.g. "near-term EPS dilutive due to equity issuance but ROE accretive over 2‚Äì3 years if synergies are realised").
  ‚Ä¢ Where helpful, you may also use web_search to understand the company‚Äôs current scale and sector context, but do not invent precise targets or numerical forecasts that are not supported.
  ‚Ä¢ Only if you genuinely cannot infer any economic impact (for example, a purely procedural or non-material filing), say "Not enough information".

- "Expected Triggers Ahead":
  ‚Ä¢ List 2‚Äì5 key upcoming milestones or triggers investors should track
    (regulatory approvals, completion timelines, integration progress, further disclosures, etc.).

- "Risks":
  ‚Ä¢ 2‚Äì5 concise risk points (regulatory, execution, financing, integration, dilution, etc.).

- "Probability Assessment":
  ‚Ä¢ Classify overall likelihood of successful completion of the core event as one of:
    "Low", "Medium", or "High".

- "Regulations":
  ‚Ä¢ If the PDF explicitly cites regulations, reproduce them exactly
    (e.g. "Regulation 30 of SEBI (LODR) Regulations, 2015").
  ‚Ä¢ Otherwise, infer likely applicable regulations (e.g. Reg. 30, Reg. 33, SEBI (PIT), SEBI (ICDR)) and list them.

- "Source Documents":
  ‚Ä¢ Use exactly the following URLs from context:
    PDF: {pdf_url or 'NA'}
    BSE Announcement: {news_url or 'NA'}
  ‚Ä¢ Return them in a single string, e.g.:
    "PDF: {pdf_url or 'NA'}; BSE: {news_url or 'NA'}".

{style_hint}

Output format ‚Äî return ONLY valid JSON with this exact structure (no prose, no markdown):

{{
  "table": [
    {{
      "Company": "{company or 'NA'}",
      "Market Cap (‚Çπ Cr)": "<final market cap in ‚Çπ Cr or 'NA'>",
      "Nature of Event": "<nature>",
      "Announcement Type From PDF": "<announcement type>",
      "Key Announcement Components": "<key components>",
      "Timeline (announcement ‚Üí approvals ‚Üí execution)": "<timeline>",
      "Stakeholder Impact": "<stakeholder impact>",
      "Valuation Impact": "<valuation impact>",
      "Expected Triggers Ahead": "<expected triggers>",
      "Risks": "<risks>",
      "Probability Assessment": "<Low|Medium|High>",
      "Regulations": "<regulation text>",
      "Source Documents": "PDF: {pdf_url or 'NA'}; BSE: {news_url or 'NA'}"
    }}
  ]
}}

Context:
Company: {company or 'NA'}
Headline: {headline or 'NA'}
Subcategory: {subcat or 'NA'}
PDF URL: {pdf_url or 'NA'}
BSE Announcement URL: {news_url or 'NA'}
Current Market Cap Placeholder (may be 'NA'): {market_cap}
"""

    resp = client.responses.create(
        model=model,
        temperature=temperature,
        max_output_tokens=max_output_tokens,
        # enable web search tool
        tools=[{"type": "web_search"}],
        input=[{
            "role": "user",
            "content": [
                {"type": "input_text", "text": task},
                {"type": "input_file", "file_id": fobj.id},
            ],
        }],
    )
    return (getattr(resp, "output_text", None) or "").strip()

def safe_summarize(*args, **kwargs) -> str:
    """Simple rate-limit-friendly wrapper around summarize_pdf_with_openai."""
    for i in range(4):
        try:
            return summarize_pdf_with_openai(*args, **kwargs)
        except Exception as e:
            msg = str(e)
            if "429" in msg or "rate" in msg.lower():
                time.sleep(2.0 * (i + 1))
                continue
            raise
    return "‚ö†Ô∏è Unable to summarize due to repeated rate limits."

# =========================================
# Sidebar controls
# =========================================
with st.sidebar:
    st.header("‚öôÔ∏è Controls")
    today = datetime.now().date()
    start_date = st.date_input(
        "Start date",
        value=today - timedelta(days=1),
        max_value=today
    )
    end_date = st.date_input(
        "End date",
        value=today,
        max_value=today,
        min_value=start_date
    )

    model = st.selectbox(
        "OpenAI model",
        ["gpt-4.1-mini", "gpt-4o-mini", "gpt-4.1"],
        index=0,
        help="Models with vision/file-reading capability. 4.1-mini/4o-mini are cost-efficient."
    )
    style = st.radio("Summary style", ["bullets", "narrative"], horizontal=True)
    max_tokens = st.slider("Max output tokens", 200, 2000, 800, step=50)
    temperature = st.slider("Creativity (temperature)", 0.0, 1.0, 0.2, step=0.1)

    max_workers = st.slider(
        "Parallel summaries", 1, 8, 3,
        help="Lower if you hit 429s."
    )
    max_items = st.slider(
        "Max announcements to summarize", 5, 200, 60, step=5
    )

    run = st.button("üöÄ Fetch & Summarize with OpenAI", type="primary")

# =========================================
# Run pipeline (fetch ‚Üí PDFs ‚Üí OpenAI summaries)
# =========================================
def _fmt(d: datetime.date) -> str:
    return d.strftime("%Y%m%d")

def _pick_company_cols(df: pd.DataFrame) -> tuple[str, str]:
    nm = _first_col(df, ["SLONGNAME", "SNAME", "SC_NAME", "COMPANYNAME"]) or "SLONGNAME"
    subcol = _first_col(
        df,
        ["SUBCATEGORYNAME", "SUBCATEGORY", "SUB_CATEGORY", "NEWS_SUBCATEGORY"]
    ) or "SUBCATEGORYNAME"
    return nm, subcol

if run:
    if not os.getenv("OPENAI_API_KEY") and "OPENAI_API_KEY" not in st.secrets:
        st.error(
            "Missing OPENAI_API_KEY (set env var, add to Streamlit Secrets, or export in your shell)."
        )
        st.stop()

    start_str, end_str = _fmt(start_date), _fmt(end_date)

    with st.status("Fetching announcements‚Ä¶", expanded=True):
        df_hits = fetch_bse_announcements_strict(
            start_str, end_str, verbose=False
        )
        st.write(f"Matched rows after filters: **{len(df_hits)}**")

    if df_hits.empty:
        st.warning("No matching announcements in this window.")
        st.stop()

    if len(df_hits) > max_items:
        df_hits = df_hits.head(max_items)

    # Build list of PDF targets
    rows = []
    for _, r in df_hits.iterrows():
        urls = _candidate_urls(r)
        rows.append((r, urls))

    st.subheader("üìë Summaries (OpenAI)")
    nm, subcol = _pick_company_cols(df_hits)

    all_tables = []  # üëà collect all row tables here for Excel

    # Worker to download and summarize a single row
    def worker(idx, row, urls):
        # try urls in order until one downloads
        pdf_bytes, used_url = None, ""
        for u in urls:
            try:
                data = _download_pdf(u, timeout=25)
                if data and len(data) > 500:
                    pdf_bytes, used_url = data, u
                    break
            except Exception:
                continue

        if not pdf_bytes:
            return idx, used_url, "‚ö†Ô∏è Could not fetch a valid PDF."

        company = str(row.get(nm) or "").strip()
        headline = str(row.get("HEADLINE") or "").strip()
        subcat = str(row.get(subcol) or "").strip()
        news_url = _primary_bse_url(row)

        # Placeholder for now; model will override using web_search where possible
        market_cap = "NA"

        summary = safe_summarize(
            pdf_bytes,
            company,
            headline,
            subcat,
            used_url,          # pdf_url
            news_url,          # exchange URL
            market_cap=market_cap,
            model=model,
            style=("bullets" if style == "bullets" else "narrative"),
            max_output_tokens=int(max_tokens),
            temperature=float(temperature),
        )
        return idx, used_url, summary

    # Run with limited parallelism
    with ThreadPoolExecutor(max_workers=max_workers) as ex:
        futs = [
            ex.submit(worker, i, r, urls)
            for i, (r, urls) in enumerate(rows)
        ]
        for fut in as_completed(futs):
            i, pdf_url, summary = fut.result()
            r = rows[i][0]
            company = str(r.get(nm) or "").strip()
            dt = str(r.get("NEWS_DT") or "").strip()
            subcat = str(r.get(subcol) or "").strip()
            headline = str(r.get("HEADLINE") or "").strip()

            # Optional context header per summary (uncomment if you like):
            # st.markdown(f"### {company}")
            # st.caption(f"{dt} ¬∑ {headline} ¬∑ {subcat}")

            # Render table for this summary
            _render_bordered_table_from_json(summary, key="table")

            # Also collect for Excel download
            df_one = _parse_table_from_json(summary, key="table")
            if df_one is not None and not df_one.empty:
                all_tables.append(df_one)

    # üëâ After all summaries, offer a combined Excel download
    if all_tables:
        combined_df = pd.concat(all_tables, ignore_index=True)
        buffer = io.BytesIO()
        with pd.ExcelWriter(buffer, engine="xlsxwriter") as writer:
            combined_df.to_excel(writer, index=False, sheet_name="Summaries")
        buffer.seek(0)

        st.download_button(
            label="üíæ Download all summaries as Excel",
            data=buffer,
            file_name=f"bse_summaries_{start_str}_{end_str}.xlsx",
            mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
        )

else:
    st.info(
        "Pick your date range and click **Fetch & Summarize with OpenAI**. "
        "This version uploads each PDF to OpenAI, lets the model web-search "
        "for latest market cap and missing details, and renders the model‚Äôs "
        "structured summary (with event nature, valuation impact based on "
        "its own judgement, triggers, risks, probability, and source docs) "
        "right here. You can then download all rows as an Excel file."
    )
